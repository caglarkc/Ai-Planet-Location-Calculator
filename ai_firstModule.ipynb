{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Cihazı ayarlayın (GPU varsa CUDA, yoksa CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Kullanılan cihaz:\", device)\n",
    "\n",
    "# Verileri yükle\n",
    "file_path_first = \"C:/Users/alica/Desktop/10k_data_first.txt\"\n",
    "file_path_second = \"C:/Users/alica/Desktop/10k_data_second.txt\"\n",
    "\n",
    "# İlk veri setini yükle\n",
    "first_data = pd.read_csv(\n",
    "    file_path_first,\n",
    "    delimiter=\"_\",\n",
    "    names=[\"M\", \"Peri\", \"Node\", \"Incl.\", \"E\", \"N\", \"A\", \"Epoch\", \"x\", \"y\", \"z\"]\n",
    ")\n",
    "\n",
    "# İkinci veri setini yükle (2026 tarihindeki gerçek konumlar)\n",
    "second_data = pd.read_csv(\n",
    "    file_path_second,\n",
    "    delimiter=\"_\",\n",
    "    names=[\"x_2026\", \"y_2026\", \"z_2026\"]\n",
    ")\n",
    "\n",
    "# Epoch tarihini `days_since_epoch` olarak hesaplayın\n",
    "first_data[\"Epoch\"] = pd.to_datetime(first_data[\"Epoch\"], format='%Y%m%d')\n",
    "target_date = pd.Timestamp(\"2026-01-01\")\n",
    "first_data[\"days_since_epoch\"] = (target_date - first_data[\"Epoch\"]).dt.days\n",
    "\n",
    "# Özellikleri ve hedef değerleri ayır\n",
    "X = first_data[[\"M\", \"Peri\", \"Node\", \"Incl.\", \"E\", \"N\", \"A\", \"days_since_epoch\"]].values\n",
    "y_2026 = second_data[[\"x_2026\", \"y_2026\", \"z_2026\"]].values  # 2026 hedef konumlar\n",
    "\n",
    "# Eğitim ve test setlerine bölme\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_2026, test_size=0.2, random_state=42)\n",
    "\n",
    "# Veriyi ölçeklendir\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "\n",
    "# PyTorch tensörlerine dönüştür ve GPU'ya taşı\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# PyTorch DataLoader ile veri kümelerini oluştur\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Modeli oluştur\n",
    "class PositionNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PositionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = PositionNN(input_size=X_train.shape[1], output_size=3).to(device)\n",
    "\n",
    "# Optimizasyon ve düzenlileştirme\n",
    "criterion = nn.SmoothL1Loss()  # Daha stabil kayıp fonksiyonu\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Doğrulama seti üzerinde değerlendirme\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Tüm veri üzerinde tahmin yap\n",
    "X_tensor = torch.tensor(scaler_X.transform(X), dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions_scaled = model(X_tensor)\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled.cpu().numpy())\n",
    "\n",
    "# Ortalama hata hesaplama\n",
    "errors = np.abs(predictions - y_2026)\n",
    "mean_error = np.mean(errors)\n",
    "print(f\"Ortalama hata (AU): {mean_error:.4f}\")\n",
    "\n",
    "# Ortalama gerçek değer büyüklüğü\n",
    "mean_true_magnitude = np.mean(np.abs(y_2026))\n",
    "\n",
    "\n",
    "\n",
    "# Tahminleri ve gerçek değerleri karşılaştırma\n",
    "comparison = pd.DataFrame({\n",
    "    \"x_pred\": predictions[:, 0],\n",
    "    \"y_pred\": predictions[:, 1],\n",
    "    \"z_pred\": predictions[:, 2],\n",
    "    \"x_true\": y_2026[:, 0],\n",
    "    \"y_true\": y_2026[:, 1],\n",
    "    \"z_true\": y_2026[:, 2]\n",
    "})\n",
    "\n",
    "# Sonuçları CSV formatında kaydet\n",
    "comparison.to_csv(\"C:/Users/alica/Desktop/comparison_results_fixed.csv\", index=False)\n",
    "print(\"Sonuçlar 'comparison_results_fixed.csv' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki blok sayesınde yapay zeka %15 - %20 doğruluk oranı ile istenilen veriyi hesaplamaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosya yolu ve çıktı dosyası\n",
    "file_path = \"C:/Users/alica/Desktop/comparison_results_fixed.csv\"  # Giriş dosyasının yolu\n",
    "output_file = \"C:/Users/alica/Desktop/caglar.csv\"  # Çıktı dosyasının adı\n",
    "\n",
    "# Verileri yükle\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Farkları hesapla ve yüzdelik farkları bul\n",
    "data['x_diff'] = ((data['x_pred'] - data['x_true']) / data['x_true']) * 100\n",
    "data['y_diff'] = ((data['y_pred'] - data['y_true']) / data['y_true']) * 100\n",
    "data['z_diff'] = ((data['z_pred'] - data['z_true']) / data['z_true']) * 100\n",
    "\n",
    "# NaN ve sonsuz değerleri temizle\n",
    "data['x_diff'] = data['x_diff'].replace([float('inf'), -float('inf')], 0).fillna(0)\n",
    "data['y_diff'] = data['y_diff'].replace([float('inf'), -float('inf')], 0).fillna(0)\n",
    "data['z_diff'] = data['z_diff'].replace([float('inf'), -float('inf')], 0).fillna(0)\n",
    "\n",
    "# Ortalama yüzdelik fark hesaplama\n",
    "data['average_percent'] = (\n",
    "    data[['x_diff', 'y_diff', 'z_diff']].mean(axis=1)\n",
    ")\n",
    "\n",
    "# %70'ten büyük hata paylarını tespit et\n",
    "high_error_rows = data[data['average_percent'].abs() > 70]\n",
    "filtered_data = data[data['average_percent'].abs() <= 70]  # Genel hata için filtrelenmiş veriler\n",
    "\n",
    "# %70'ten büyük hata paylarının sayısını bul\n",
    "high_error_count = len(high_error_rows)\n",
    "print(f\"%70'den büyük hata payına sahip {high_error_count} satır bulundu.\")\n",
    "\n",
    "# Genel ortalama hata yüzdesini filtrelenmiş verilerle hesapla\n",
    "if len(filtered_data) > 0:\n",
    "    total_average_error = (\n",
    "        filtered_data[['x_diff', 'y_diff', 'z_diff']].abs().sum().sum() / (len(filtered_data) * 3)\n",
    "    )\n",
    "    print(f\"Genel Ortalama Hata Payı (filtrelenmiş): {total_average_error:.1f}%\")\n",
    "else:\n",
    "    print(\"Filtrelenmiş veri bulunmamaktadır, genel hata payı hesaplanamadı.\")\n",
    "\n",
    "# Ortalama yüzdelik farkları tam sayı ve bir ondalık hane olacak şekilde yuvarla\n",
    "data['average_percent'] = data['average_percent'].round(1).astype(str) + \"%\"\n",
    "\n",
    "# Sadece `average_percent` sütununu kaydet\n",
    "average_percentage = data[['average_percent']]\n",
    "average_percentage.to_csv(output_file, index=False, header=False)\n",
    "\n",
    "print(f\"Sadece ortalama yüzdelik farklarla dosya '{output_file}' olarak kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki kod ile yapay zekanın yüzde kaç doğru eğitildiğini ve sonuclarnın yüzde kaç dogru oldugnu hesaplıyoruz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
